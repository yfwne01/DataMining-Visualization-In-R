---
title: "IE7275 HW3 Group 5"
author: "Group 5"
date: "4/01/2020"
output:
  html_document:
    df_print: paged
    fig_height: 7
    fig_width: 10
    highlight: tango
    theme: readable
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r, message=FALSE}
#load packages
library(readr)
library(readxl)
library(forecast)
library(tidyverse)
library(caret)
library(rpart)
library(caret)
library(e1071)
library(data.table)
library(leaps)
library(MASS)
library(readr)
library(corrplot)
library(gridExtra)
library(formattable)
library(FNN)
```
### Problem 7.1
```{r}
#Problem 7.1
#read the dataset
UniversalBank <- read_csv("UniversalBank.csv")
str(UniversalBank)

#check the missing values
sapply(UniversalBank, function(x) sum(is.na(x)))

# drop the irrelevant columns 
UniversalBank$ID <- NULL
UniversalBank$`ZIP Code` <- NULL

## Remove spaces from column names
university01<- setnames(x = UniversalBank, old = names(UniversalBank),
         new = gsub(" ", "", names(UniversalBank)))

# get the chategorical response variable
university01$PersonalLoan <- factor(university01$PersonalLoan)

#reorder the columns
university02 <- university01[,c(8,1,2,3,4,5,6,7,9,10,11,12)]
```

Experience: # of years of professional experience
Income: annual income (*$000)
Family: family size
CCAvg: Average monthly credit card spending ($000)
Education Education level: 1: undergrad; 2, Graduate; 3; Advance/Professional 
Mortgage: Value of house mortgage ($000)
 (response variable)
Personal loan :Did this customer accept the personal loan offered in he last campaign? 1, yes; 0, no 

1: yes and 0: no
Securities Acct: Does the customer have a securities account with the bank? 
CD Account: Does the customer have a certifcate of deposit (CD) account with the bank? 
Online: Does the customer use internet bank facilities? 
CreditCard: Does the customer use a credit card issued by the Bank?


```{r}
#part a
#load and partition the dataset: training (60%) and validation (40%) sets
set.seed(105)
indexknn<- sample(1:nrow(university02),size=nrow(university02)*0.6,replace = FALSE) 
train_knn<- university02[indexknn,] # 60% training data
test_knn<- university02[-indexknn,]

#create the separate dataframe
train_knn_pl<- university02[indexknn,1]

# initialize normalized training, validation data, complete data frames to originals
train.norm.df <- train_knn
valid.norm.df <- test_knn
bank.norm.df <- university02

# use preProcess() from the caret package to normalize features
norm.values <- preProcess(train_knn[, -1], method=c("center", "scale"))

train.norm.df[, -1] <- predict(norm.values, train_knn[, -1])
valid.norm.df[, -1] <- predict(norm.values, test_knn[, -1])
bank.norm.df[, -1] <- predict(norm.values, university02[, -1])

```


```{r}
# for the new customer; create the data frame
library(dplyr)
set.seed(105)
new.df1<-data.frame(40,10,84,2,2,1,0,0,0,1,1) %>%
        setNames(names(university02[, -1]))

new.df2<-data.frame(40,10,84,2,2,0,0,0,0,1,1) %>%
        setNames(names(university02[, -1]))

#normalize
new.norm.df1 <- predict(norm.values, new.df1)

#build the prediction model
cl <- train_knn_pl[,1,drop = TRUE]

knn.pred_new1<-knn(train = train.norm.df[,-1],                          
                         test = new.norm.df1,                          
                         cl, k = 1)
row.names(train_knn)[attr(knn.pred_new1,"nn.index")]

```


a.**How would this customer be classified?** 
when k=1, the closest customer in the train dataset locates at the number 176 row, and the personloan status is 0. Therefore, for this new customer, he will be classified as the class that he will not accept the personal loan offered in his last campaign. 

b.**What is a choice of k that balances between overfitting and ignoring the predictor information?** 
Generally speasking, if k is too low, we may be fitting the noise in the data, and if k is too high, we may miss out on the method`s ability to capture the local structure in the data. If we want to balance between overfitting to the prodictor information and ignore the  predictor information, we would consider the extreme condition that k is the same number of records in the training dataset. we simply assign all records to the majority class in the training data, oversmoothing the absense of useful information in the predict about the class


```{r}
#part c 
#compute knn for different k on validation to find the best k
# initialize a data frame with two columns: k, and accuracy
set.seed(105)
i=1                          
k.optm=1                     
for (i in 1:20){ 
    knn.mod <-  knn(train=train.norm.df[,-1], test=valid.norm.df[, -1], cl, k=i)
    k.optm[i] <- 100 * sum(knn.mod == test_knn$PersonalLoan)/NROW(test_knn$PersonalLoan)
    k=i  
    cat(k,'=',k.optm[i],'\n')
}

```


```{r}
#part c
#the best k=5, with the highest accuracy
knn.5 <- knn(train=train.norm.df[,-1], test=valid.norm.df[, -1], cl, k=5)

#show the confusion matrix for the validation data
library(caret)
confusionMatrix(knn.5,valid.norm.df$PersonalLoan)
```


```{r}
#part d
set.seed(1060)
knn.pred_new1<-knn(train = train.norm.df[,-1],                          
                         test = new.norm.df1,                          
                         cl, k = 5)
knn.pred_new1
row.names(train_knn)[attr(knn.pred_new1,"nn.index")]
```

d. **Classify the customer using the best k.** 
when k=5, the closest customer in the train dataset locates at the number 176,1731,2613,2921,and 2713 rows, and the personloan status is 0. Therefore, for this new customer, he will be classified as the class that he will not accept the personal loan offered in his last campaign.

```{r}
#part e
#Repartition the data, this time into training, validation, and test sets (50% : 30% : 20%).
set.seed(1050)
splitSample <- sample(1:3, size=nrow(university02), prob=c(0.5,0.3,0.2), replace = TRUE)
train.hex <- university02[splitSample==1,]
valid.hex <- university02[splitSample==2,]
test.hex <- university02[splitSample==3,]

#create the separate dataframe for personloan
train.hex_pl <- university02[splitSample==1,1]

```


```{r}
# initialize normalized training, validation data, complete data frames to originals
train.norm.df <- train.hex
valid.norm.df <- valid.hex
test.norm.df <- test.hex

# use preProcess() from the caret package to normalize features
set.seed(1050)
norm.values <- preProcess(train.hex[, -1], method=c("center", "scale"))
train.norm.df[, -1] <- predict(norm.values, train.hex[, -1])
valid.norm.df[, -1] <- predict(norm.values, valid.hex[, -1])
test.norm.df[, -1] <- predict(norm.values, test.hex[, -1])

```


```{r}
#apply the k-NN method with the k=5
set.seed(500)
cl <- train.hex_pl[,1,drop = TRUE]

model1 <- knn(train=train.norm.df[,-1], test=valid.norm.df[, -1], cl, k=5)
model2 <- knn(train=train.norm.df[,-1], test=test.norm.df[, -1], cl, k=5)

#show the confusion matrix for the validation data
library(caret)
confusionMatrix(model1,valid.norm.df$PersonalLoan)

#show the confusion matrix for the test data
library(caret)
confusionMatrix(model2,test.norm.df$PersonalLoan)
```


e.**Compare the confusion matrix of the test set with that of the training and validation sets. Comment on the differences and their reason.** 
Train the knn model with training dataset and k=5. When we used the validation set to test the model, the model accuracy is 0.944. When we used the test set to test the model, the model accuracy is 0.95, which is higher than the previous accuracy. KNN is a lazy learner. For every record to be predicted, we compute its distance from the entire set of trainging records. Because the validation data size is larger than the testing data size, the error for validation the model should be higher if we choose the same training dataset.

### Problem 7.2
```{r}
#problem7.2
#load the dataset 
library(readr)
housing <- read_csv("BostonHousing.csv")
str(housing)

#check the missing values
sapply(housing, function(x) sum(is.na(x)))

#ignore the irrelevant column
housing$`CAT. MEDV` <- NULL
```


```{r}
#part a
#load and partition the dataset: training (60%) and validation (40%) sets
set.seed(500)
indexknn<- sample(1:nrow(housing),size=nrow(housing)*0.6,replace = FALSE) 
train_knn <- housing[indexknn,] # 60% training data
test_knn<- housing[-indexknn,]

#create the separate dataframe for MEDV feature 
train_knn_MEDV<- housing[indexknn,13]
test_knn_MEDV<- housing[-indexknn,13]

# initialize normalized training, validation data, complete data frames to originals
train.norm.df <- train_knn
valid.norm.df <- test_knn
housing.norm.df <-housing

# use preProcess() from the caret package to normalize features
norm.values <- preProcess(train_knn[, -13], method=c("center", "scale"))

train.norm.df[, -13] <- predict(norm.values, train_knn[, -13])
valid.norm.df[, -13] <- predict(norm.values, test_knn[, -13])
housing.norm.df[, -13] <- predict(norm.values, housing[, -13])

```


```{r}
library(class)
library(caret)
library(FNN)
#initialize a data frame with two columns: k, and accuracy
#trying values of k from 1 to 5
set.seed(105)
accuracy.df <- data.frame(k = seq(1, 5, 1), RMSE = rep(0, 5))

# compute knn for different k on validation. Column 13 is MEDV
#train-matrix or data frame of training set cases.
#test-matrix or data frame of test set cases. A vector will be interpreted as a row vector for a single case.
#cl-actor of true classifications of training set

cl <- train_knn_MEDV[,1,drop = TRUE]
for(i in 1:5){
  knn.pred<-class::knn(train = train.norm.df[,-13],                          
                         test = valid.norm.df[,-13],                          
                         cl, k = i)
  accuracy.df[i,2]<-RMSE(as.numeric(as.character(knn.pred)),test_knn$MEDV)
}

accuracy.df
```

a.**What is the best k? What does it mean?**
 In general, the RMSE corresponds to the square root of the average difference between the observed known outcome values and the predicted values. The lower the RMSE, the better the model performance. Therefore, for a KNN for a numerical outcome, the best k is the number that provides smmoothing that reduces the risk of overfitting due to noise in the training data, with the lowest RMSE value. In this case, k=2 is the best k with the lowest RMSE (5.63)


```{r}
#part b
#predict the MEDV for a tract with the following information, using the best k:
library(dplyr)
set.seed(1015)
#New data
new.df<-data.frame(0.2,0,7,0,0.538,6,62,4.7,4,307,21,10) %>%
        setNames(names(housing[, -13]))
#normalize
new.norm.df <- predict(norm.values, new.df)

#build the prediction model
knn.pred_new<-class::knn(train = train.norm.df[,-13],                          
                         test = new.norm.df,                          
                         cl, k = 2)
accuracy.df_new <- data.frame(MEDV = knn.pred_new, RMSE = RMSE(as.numeric(as.character(knn.pred_new)),test_knn$MEDV))
accuracy.df_new

```


b.**Predict the MEDV for a tract with the following information, using the best k: **
The predicted MEDV is 18.5 and the corresponding RMSE is 9.54

```{r}
#part c
#build the prediction model
set.seed(100)
cl <- train_knn_MEDV[,1,drop = TRUE]
knn.pred<-class::knn(train = train.norm.df[,-13],                          
                         test = train.norm.df[,-13],                          
                         cl, k = 2)
#get the error of the training set
a <-RMSE(as.numeric(as.character(knn.pred)),train_knn$MEDV)
a
```


c.**If we used the above k-NN algorithm to score the training data, what would be the error of the training set?**
Training error here is the error you'll have when you input your training set to your KNN as test set. As you can see, when I input the training set as the test set, the error of the training set is 3.26, which is overly optimistic compared to the error rate when applying this k-NN predictor to the test set (5.63). <br>

d.**Why is the validation data error overly optimistic compared to the error rate when applying this k-NN predictor to new data?**
Since your test sample is in the training dataset, it'll choose itself as the closest and never make mistake. For this reason, the training error will be zero when K = 1, irrespective of the dataset. In our case, k=2, which means that the model will choose itself and one closest to itself for the validation, therefore, If we used the k-NN algorithm to score the training data itself, we will get optimistic errors. <br>

e.**If the purpose is to predict MEDV for several thousands of new tracts, what would be the disadvantage of using k-NN prediction? List the operations that the algorithm goes through in order to produce each prediction.**
 <br>

As KNN is a lazy-learning algorithm, it's slow as it stores the training data and calculates based on the current data set instead of coming up with an algorithm based on historical data. If we were to work with MEDV for several thousands of new tracts, the prediction stage might be very slow. In other words, the algorithm must compute the distance and sort all the training data at each prediction, which can be even slower when there is a large number of training data. The operation steps are as following:
  
  1: The algorithm finds distance by computing the distances between the new data and all the training data. Mostly used metrics for calculating distance are Euclidean, Manhattan and Minkowski.
  
  2: Then it finds a minimum distance, sort in order, and determine k nearest neighbors based on minimum distance values and cross validated RMSE values
  
  3: The category of the nearest neighbors are then analyzed and assigned to the test data based on highest majority/weight
  
  4: The predicted class label is returned.
</p> 
 
 <br>
 
### Problem 8.1

```{r}
#problem 8.1
#read the dataset
UniversalBank <- read_csv("UniversalBank.csv")

#check the missing values
sapply(UniversalBank, function(x) sum(is.na(x)))

# drop the irrelevant columns 
UniversalBank$ID <- NULL
UniversalBank$`ZIP Code` <- NULL

## Remove spaces from column names
university01<- setnames(x = UniversalBank, old = names(UniversalBank),
         new = gsub(" ", "", names(UniversalBank)))

#reorder the columns
university02 <- university01[,c(8,1,2,3,4,5,6,7,9,10,11,12)]

#convert variables to categorical type
for (i in c(1:dim(university02)[2])){
  university02[,i] = data.frame(apply(university02[i], 2, as.factor))
}
str(university02)

```

```{r}
#get the two predictor variables and response variable
university03 <- university02[,c("PersonalLoan","Online","CreditCard")]

#partition the data into training (60%) and validation (40%) sets
set.seed(1005)
index<- sample(1:nrow(university03),size=nrow(university03)*0.6,replace = FALSE) 
train_data<- university03[index,] # 60% training data
test_data<- university03[-index,]

```

```{r}
#a create the pivot table
set.seed(500)
#1 use the reshape
library(reshape)
mdata <- melt(train_data, id=c("Online","CreditCard"))
cast(mdata, Online + CreditCard ~ variable)

#2 use the pivot table
library(rpivotTable)
rpivotTable::rpivotTable(train_data,rows = c("CreditCard", "Online"), cols = "PersonalLoan", 
            width = "100%", height="400%")

```

```{r}
#b
p <- round(52/568,5)
p
paste("The exact bayes conditional probability that this customer will accept the loan offer:", p*100, "%")
```

```{r}
#c
#pivot table 1:  Loan (rows) as a function of Online (columns) 
library(rpivotTable)
rpivotTable::rpivotTable(train_data, rows = "PersonalLoan", cols = "Online", 
            width = "100%", height="400%")
```

```{r}
#c
#pivot table 2:Loan (rows) as a function of CC
library(rpivotTable)
rpivotTable(train_data, rows = "PersonalLoan", cols = "CreditCard", 
            width = "100%", height="400%")
```

```{r}
#d
p1 <- round(86/279,4)
paste("P(CC  =  1 ∣Loan  =  1) =", p1*100,"%")

p2<- round(167/279,4)
paste("P(Online = 1 ∣Loan = 1)=", p2*100,"%")

p3<- round(279/3000,4)
paste("P(Loan = 1)=", p3*100,"%")

p4 <- round(865/2721,4)
paste("P(CC= 1 ∣Loan = 0)=", p4*100,"%")

p5 <- round(1610/2721,4)
paste("P(Online = 1 ∣Loan = 0)=", p5*100,"%")

p6 <- round(2721/3000,4)
paste("P(Loan = 0)=", p6*100,"%")
```

e.P(Loan = 1 ∣CC = 1, Online = 1) = P(CC =1 | L=1)P(Online=1 | L=1)P(L=1) /(P(CC =1 | L=1) P(Online=1 | L=1) P(L=1)+P(CC =1 | L=0) P(Online=1 | L=0) P(L=0))
```{r}
#e
#compute the naive bayes probability
                 
p <- round((p1*p2*p3)/((p1*p2*p3)+(p4*p5*p6)),5)
paste("P(Loan = 1 ∣CC = 1, Online = 1) =", p*100,"%")

```


f.**Compare this value with the one obtained from the pivot table in (b). Which is a more accurate estimate?**
From (b), the exact bayes conditional probability that this customer will accept the loan offer is 9.155%".
From (e), the naive bayes conditional probability that this customer will accept the loan offer is 9.138%.
The redsult from (e) is more accurate than the result from (b) because we used the entire training dataset to calculate the condistional probabilities in part (e), and we no longer restrict the probablity calculation to those records that match the resord to be classified.

```{r}
#g
#run the NB model on training data
nb_university <-naiveBayes(PersonalLoan ~ CreditCard + Online, train_data)
nb_university

pred_prob <- predict(nb_university, newdata = train_data, type = "raw")


#find the entry that corresponds to P(Loan = 1 ∣ CC = 1, Online = 1)
df <- data.frame(CC=train_data$CreditCard,
                 Online=train_data$Online,actual=train_data$PersonalLoan,pred_prob)
df1 <- df %>% filter(CC=="1",Online=="1",actual=="1")
df1
```


g.**Examine the model output on training data, and find the entry that corresponds to P(Loan = 1 ∣ CC = 1, Online = 1).Compare this to the number you obtained in (e)**
As you can see the X1 from table df1, which represents the probability(P(Loan = 1 ∣ CC = 1, Online = 1)). The probability is around 9.138%.
Compare this to the result from part (e) (probability is 9.138%), we can make the conclusion that naive bayes conditional probability is very accurate.

### Problem 8.2

```{r}
#problem 8.2
#NB is only suitful for categorical predictor variables
library(readr)
accident <- read_csv("Accidents.csv")

#insert a column INJURY
accident1 <- mutate(accident, INJURY = ifelse(accident$MAX_SEV == "no-injury", 0, 1))

#convert variables to categorical type
for (i in c(1:dim(accident1)[2])){
  accident1[,i] = data.frame(apply(accident1[i], 2, as.factor))
}
str(accident1)

```

```{r}
#part a 
library(dplyr)
paste("Total number of Non-injury:" , 
      length(which(accident$MAX_SEV == "no-injury")))
paste("Total number of Injury:", 
      length(which(accident$MAX_SEV == "non-fatal" | accident$MAX_SEV == "fatal")))

#calculate the probability of injury
a <- 308/(308+292)*100
a
```


a.**Using the information in this dataset, if an accident has just been reported and no further information is available, what should the prediction be? (INJURY = Yes or No?) Why?**
Since ~51% of the accidents in our data set resulted in an accident, we should predict that an accident will result in injury because it is slightly more likley.

```{r}
#part b (i)
#create a new subset with only the required records
accident2 <- accident1[1:12, c("INJURY", "WEATHER_adverse", "TRAF_two_way")]

#install.packages("rpivotTable")
library(rpivotTable)
rpivotTable::rpivotTable(accident2,rows = "INJURY", cols = c("WEATHER_adverse","TRAF_two_way"), 
            width = "100%", height="400%")
```

```{r}
#part b (ii)
#Exact Bayes Conditional Probabilities of an injury

#1 P(Injury=1|WEATHER_R = 1, TRAF_CON_R =0):
p1 <- 2/5
p1
#2 P(Injury=1|WEATHER_R = 1, TRAF_CON_R =1):
p2 <- 0
p2

#3 P(Injury=1|WEATHER_R = 0, TRAF_CON_R =0):
p3 <- 2/5
p3

#4 P(Injury=1|WEATHER_R = 0, TRAF_CON_R =1):
p4 <- 1
p4
```


```{r}
#part b (iii)
#Classify the 12 accidents using these probabilities and a cutoff of 0.5
#Insert the probability according to the result above
accident2$Prob_INJURY <- c(0.6,1,0.6,0.6,0.4,0.4,0.6,1,0.6,0.4,0.6,0.4)
#Insert the prediction according to the pro_INJURY with a cutoff of 0.5
accident2 <- mutate(accident2, Predict_class = ifelse(Prob_INJURY>0.5, 1, 0))
accident2$Predict_class <- as.factor(accident2$Predict_class)
str(accident2)
```


```{r}
#part b (vi)
#Compute manually the naive Bayes conditional probability of an injury given WEATHER_R = 1 and TRAF_CON_R = 1.
p <- (1/3)*(1/3)*(1/2)/((1/3)*(1/3)*(1/2)+(1/2)*0*(1/2))
p
```


```{r}
#part b (v)
#Run a naive Bayes classifier on the 12 records
set.seed(105)
library(e1071)
accident2_nb <- naiveBayes(INJURY ~ WEATHER_adverse + TRAF_two_way, accident2)

#Check the model output to obtain probabilities and classifications for all 12 records
pred_prob <- predict(accident2_nb, newdata = accident2, type = "raw")
pred_class <- predict(accident2_nb, newdata = accident2)

#Compare this to the exact Bayes classification
library(caret)
confusionMatrix(pred_class,accident2$Predict_class)

#compare this with the true classification
confusionMatrix(pred_class,accident2$INJURY)
```


b.**Compare this to the exact Bayes classification. Are the resulting classifications equivalent? Is the ranking (= ordering) of observations equivalent?**
As you can see from the results of the confusion matrix, the accuracy is 50% comapring the NB model result with exact bayes classifiction, and the accuracy is 66.7% comparing the NB model result with true response classification. The resulting classifications are not equivalent and the ranking of observations are not equivalent. Since we trained on the same data we are testing, it is expected that the trained data performs better than our manual calculations.


```{r}
#part c (i)
#Choose the preditors
predictor <- c("INJURY","RushHour", "WRK_ZONE", "WKDY", "INT_HWY", "LGTCON_day", "LEVEL", "SPD_LIM", "SUR_COND_dry", "TRAF_two_way", "WEATHER_adverse")
accident3 <- accident1[,predictor]
```


```{r}
#part c (ii)
#split the dataset
set.seed(105)
train.index <- sample(c(1:dim(accident3)[1]), dim(accident3)[1]*0.6)  
train.df <- accident3[train.index,]
valid.df <- accident3[-train.index,]

nb_train <- naiveBayes(INJURY ~., data=train.df)
nb_train

#generate the confusion matrix using the train.df, the prediction and the classes
confusionMatrix(predict(nb_train, train.df),train.df$INJURY)
error_train <- 1-0.6056
paste("train error:", error_train*100, "%")
```

```{r}
#part c (iii)
#What is the overall error for the validation set
confusionMatrix(predict(nb_train, valid.df),valid.df$INJURY)

# accuracy = 0.5417
 error_valid <- 1-0.5417
paste("valid error:", error_valid*100, "%")
```

```{r}
#part c (iv) What is the percent improvement relative to the naive rule (using the validation set))
imp <-  error_valid-error_train 
paste("The percent improvement is ",scales::percent(imp,0.01))
```



part c. (v) **Examine the conditional probabilities output. Why do we get a probability of zero for P(INJURY = No ∣ SPD_LIM = 5)?**
As you can see the result from partc (iii); the output of nb_train, there is conditional probability for SPD_LIM = 5. Therefore, this probability
(P(INJURY = No ∣ SPD_LIM = 5)) is zero.








